{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_detct.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcrT8Fbo3qiq",
        "outputId": "b727eb58-30ff-40c6-a2eb-777387802ea3"
      },
      "source": [
        "!pip3 install mtcnn"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/43/abee91792797c609c1bf30f1112117f7a87a713ebaa6ec5201d5555a73ef/mtcnn-0.1.0-py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras>=2.0.0->mtcnn) (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->keras>=2.0.0->mtcnn) (1.15.0)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXnAcchk04rv"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from mtcnn import MTCNN\n",
        "import pickle\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "KJyNdgNk33Ys",
        "outputId": "17f37874-c8c2-4649-f648-b2c51fdb17f1"
      },
      "source": [
        "# load face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# load the model\n",
        "sex_model = pickle.load(open('./model/sex-model-final.pkl', 'rb'))\n",
        "age_model = pickle.load(open('./model/age-model-final.pkl', 'rb'))\n",
        "emotion_model = pickle.load(open('./model/emotion-model-final.pkl', 'rb'))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-bd2c43f321c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msex_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/sex-model-final.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mage_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/age-model-final.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0memotion_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/emotion-model-final.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './model/sex-model-final.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TbnAvqR38tk"
      },
      "source": [
        "def rgb2gray(rgb):\n",
        "\n",
        "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
        "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
        "\n",
        "    return gray\n",
        "\n",
        "def detect_face(img):\n",
        "    \n",
        "    mt_res = detector.detect_faces(img)\n",
        "    return_res = []\n",
        "    \n",
        "    for face in mt_res:\n",
        "        x, y, width, height = face['box']\n",
        "        center = [x+(width/2), y+(height/2)]\n",
        "        max_border = max(width, height)\n",
        "        \n",
        "        # center alignment\n",
        "        left = max(int(center[0]-(max_border/2)), 0)\n",
        "        right = max(int(center[0]+(max_border/2)), 0)\n",
        "        top = max(int(center[1]-(max_border/2)), 0)\n",
        "        bottom = max(int(center[1]+(max_border/2)), 0)\n",
        "        \n",
        "        # crop the face\n",
        "        center_img_k = img[top:top+max_border, \n",
        "                           left:left+max_border, :]\n",
        "        center_img = np.array(Image.fromarray(center_img_k).resize([224, 224]))\n",
        "        \n",
        "        # create predictions\n",
        "        sex_preds = sex_model.predict(center_img.reshape(1,224,224,3))[0][0]\n",
        "        age_preds = age_model.predict(center_img.reshape(1,224,224,3))[0][0]\n",
        "        \n",
        "        # convert to grey scale then predict using the emotion model\n",
        "        grey_img = np.array(Image.fromarray(center_img_k).resize([48, 48]))\n",
        "        emotion_preds = emotion_model.predict(rgb2gray(grey_img).reshape(1, 48, 48, 1))\n",
        "        \n",
        "        # output to the cv2\n",
        "        return_res.append([top, right, bottom, left, sex_preds, age_preds, emotion_preds])\n",
        "        \n",
        "    return return_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XobwAZZ538v1"
      },
      "source": [
        "# Get a reference to webcam \n",
        "video_capture = cv2.VideoCapture(0)\n",
        "\n",
        "emotion_dict = {\n",
        "    0: 'Surprise',\n",
        "    1: 'Happy', \n",
        "    2: 'Disgust',\n",
        "    3: 'Anger',\n",
        "    4: 'Sadness',\n",
        "    5: 'Fear',\n",
        "    6: 'Contempt'\n",
        "}\n",
        "\n",
        "while True:\n",
        "    # Grab a single frame of video\n",
        "    ret, frame = video_capture.read()\n",
        "\n",
        "    # Convert the image from BGR color (which OpenCV uses) to RGB color \n",
        "    rgb_frame = frame[:, :, ::-1]\n",
        "\n",
        "    # Find all the faces in the current frame of video\n",
        "    face_locations = detect_face(rgb_frame)\n",
        "\n",
        "    # Display the results\n",
        "    for top, right, bottom, left, sex_preds, age_preds, emotion_preds in face_locations:\n",
        "        # Draw a box around the face\n",
        "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
        "        \n",
        "        sex_text = 'Female' if sex_preds > 0.5 else 'Male'\n",
        "        cv2.putText(frame, 'Sex: {}({:.3f})'.format(sex_text, sex_preds), (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 1)\n",
        "        cv2.putText(frame, 'Age: {:.3f}'.format(age_preds), (left, top-25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 1)\n",
        "        cv2.putText(frame, 'Emotion: {}({:.3f})'.format(emotion_dict[np.argmax(emotion_preds)], np.max(emotion_preds)), (left, top-40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (36,255,12), 1)\n",
        "        \n",
        "    # Display the resulting image\n",
        "    cv2.imshow('Video', frame)\n",
        "\n",
        "    # Hit 'q' on the keyboard to quit!\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release handle to the webcam\n",
        "video_capture.release()\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfwrUWfR04uf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA5SUf6z04wg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkuk0PS104y1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m5GDfRR041K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}